{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2b3c4d",
   "metadata": {},
   "source": [
    "## Setup and Configuration for Multi class Effnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed360cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b2\n",
    "\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from malaria_dataset import MalariaDataset, detection_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79630615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Path: ..\\dataset\\malaria\n",
      "Train JSON Path: ..\\dataset\\malaria\\training.json\n",
      "Test JSON Path: ..\\dataset\\malaria\\test.json\n",
      "Image Path: ..\\dataset\\malaria\\images\n"
     ]
    }
   ],
   "source": [
    "root_path = os.path.join('..', 'dataset', 'malaria')\n",
    "train_json_path = os.path.join(root_path, 'training.json')\n",
    "test_json_path = os.path.join(root_path, 'test.json')\n",
    "image_path = os.path.join(root_path, 'images')\n",
    "models_dir = os.path.join('.', 'effnet_models')\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"Root Path:\", root_path)\n",
    "print(\"Train JSON Path:\", train_json_path)\n",
    "print(\"Test JSON Path:\", test_json_path)\n",
    "print(\"Image Path:\", image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c4d5e",
   "metadata": {},
   "source": [
    "## Model and Dataset Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d898ee",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8d6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetDetector(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetDetector, self).__init__()\n",
    "        # Loading pre-trained EfficientNet-B2 as the backbone\n",
    "        self.backbone = efficientnet_b2(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Replacing the final classifier with an identity layer\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # classifier for multi-label classification\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Getting class scores from the classifier\n",
    "        class_scores = self.classifier(features)\n",
    "        \n",
    "        # return the class scores\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d5e6f",
   "metadata": {},
   "source": [
    "## Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_val_funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, device, epoch, num_classes):\n",
    "    model.train()\n",
    "    running_loss, correct, total_objects = 0.0, 0, 0\n",
    "    pbar = tqdm(loader, desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    # the loss function BCEWithLogitsLoss is ideal for multi-label tasks as it combines Sigmoid + BCE.\n",
    "    # `pos_weight` forces the model to focus on rare classes by heavily penalizing mistakes on them.\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "    for images, targets_list in pbar:\n",
    "        if not images.numel(): continue # Skip empty batches\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Getting the class scores\n",
    "        class_scores = model(images)\n",
    "        \n",
    "        # Creating a placeholder for all target labels in the batch\n",
    "        all_target_labels = torch.zeros_like(class_scores).to(device)\n",
    "\n",
    "        # Populate the multi-hot encoded tensor\n",
    "        for i, target in enumerate(targets_list):\n",
    "            labels = target['labels']\n",
    "            if len(labels) > 0:\n",
    "                all_target_labels[i, labels] = 1.0\n",
    "\n",
    "        # Calculate loss for the whole batch at once\n",
    "        loss = criterion(class_scores, all_target_labels)\n",
    "\n",
    "        if images.size(0) > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Apply sigmoid to get probabilities, then threshold to get predictions\n",
    "        preds = torch.sigmoid(class_scores)\n",
    "        preds[preds >= 0.5] = 1\n",
    "        preds[preds < 0.5] = 0\n",
    "\n",
    "        # Compare if the predicted multi-hot vector exactly matches the true one\n",
    "        total_objects += images.size(0)\n",
    "        correct += (preds == all_target_labels).all(dim=1).sum().item()\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(loader) if len(loader) > 0 else 0\n",
    "    accuracy = 100 * correct / total_objects if total_objects > 0 else 0\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def validate_model(model, loader, device, category_map, return_preds=False):\n",
    "    model.eval()\n",
    "    all_labels_for_report = []\n",
    "    all_preds_for_report = []\n",
    "    \n",
    "    # Add criterion to calculate loss\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Validating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets_list in pbar:\n",
    "            if not images.numel(): continue\n",
    "            images = images.to(device)\n",
    "            \n",
    "            class_scores = model(images)\n",
    "            \n",
    "            # Create the true multi-hot labels for the batch\n",
    "            true_labels = torch.zeros_like(class_scores)\n",
    "            for i, target in enumerate(targets_list):\n",
    "                labels = target['labels']\n",
    "                if len(labels) > 0:\n",
    "                    true_labels[i, labels] = 1.0\n",
    "            \n",
    "            #   NEW: Calculate and accumulate loss  \n",
    "            loss = criterion(class_scores, true_labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.sigmoid(class_scores)\n",
    "            preds[preds >= 0.5] = 1\n",
    "            preds[preds < 0.5] = 0\n",
    "            all_preds_for_report.extend(preds.cpu().numpy())\n",
    "            all_labels_for_report.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    if return_preds:\n",
    "        return all_labels_for_report, all_preds_for_report\n",
    "    else:\n",
    "        #   MODIFIED: Return loss and accuracy  \n",
    "        epoch_loss = running_loss / len(loader) if len(loader) > 0 else 0\n",
    "        accuracy = 100 * np.mean(np.array(all_labels_for_report) == np.array(all_preds_for_report))\n",
    "        return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e6f7g",
   "metadata": {},
   "source": [
    "## Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb9f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Calculating class weights for the loss function...\n",
      "Calculated pos_weight: [ 2.5321636  7.882353  11.851064   0.         3.9508197  6.6942673\n",
      "  1.0268457]\n",
      "Found 7 classes: {'difficult': 0, 'gametocyte': 1, 'leukocyte': 2, 'red blood cell': 3, 'ring': 4, 'schizont': 5, 'trophozoite': 6}\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "#   Pre-calculation of Class Weights  \n",
    "# This is efficient because it doesn't change between runs.\n",
    "with open(train_json_path, 'r') as f:\n",
    "    train_entries = json.load(f)\n",
    "temp_ds = MalariaDataset(train_json_path, image_path) # Temp dataset to get map\n",
    "category_map = temp_ds.category_map\n",
    "num_classes = len(category_map)\n",
    "class_names = [name for name, index in sorted(category_map.items(), key=lambda item: item[1])]\n",
    "\n",
    "#   Pre-calculation of Class Weights  \n",
    "print(\"Calculating class weights for the loss function...\")\n",
    "\n",
    "# We need the training data entries and category map first\n",
    "with open(train_json_path, 'r') as f:\n",
    "    train_entries = json.load(f)\n",
    "\n",
    "# Create a multi-hot matrix to count class occurrences\n",
    "multi_hot_labels = np.zeros((len(train_entries), num_classes), dtype=float)\n",
    "for i, entry in enumerate(train_entries):\n",
    "    for obj in entry['objects']:\n",
    "        cat_idx = category_map.get(obj['category'])\n",
    "        if cat_idx is not None:\n",
    "            multi_hot_labels[i, cat_idx] = 1.0\n",
    "\n",
    "# Count the total number of positive samples for each class\n",
    "positive_counts = multi_hot_labels.sum(axis=0)\n",
    "total_samples = len(train_entries)\n",
    "\n",
    "# Calculate the pos_weight using the formula\n",
    "# A small epsilon (1e-6) is added to avoid division by zero for any class that might not appear\n",
    "pos_weight = (total_samples - positive_counts) / (positive_counts + 1e-6)\n",
    "\n",
    "# Convert to a PyTorch tensor and move to the correct device\n",
    "# This tensor is then used in the loss function (nn.BCEWithLogitsLoss)\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "print(f\"Calculated pos_weight: {pos_weight_tensor.cpu().numpy()}\")\n",
    "\n",
    "print(f\"Found {num_classes} classes: {category_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the sampler\n",
    "def create_sampler(train_ds):\n",
    "    class_counts = np.bincount([label for label in train_ds.labels if label != -1], minlength=num_classes)\n",
    "    class_weights = 1. / (class_counts + 1e-6)\n",
    "    sample_weights = np.array([class_weights[t] if t != -1 else 0 for t in train_ds.labels])\n",
    "    return WeightedRandomSampler(torch.from_numpy(sample_weights).double(), len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hyperparameter run 1/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 2/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 3/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 4/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 5/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 6/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 7/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 8/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 128, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 9/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 10/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'Adam', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 11/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 12/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.001, 'optimizer': 'SGD', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 13/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 14/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'Adam', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 15/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': 'oversample'}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': 'oversample'}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "=== Hyperparameter run 16/16 ===\n",
      "\n",
      "==================================================\n",
      "Params: {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': None}\n",
      "==================================================\n",
      "Found existing model and results for params {'batch_size': 32, 'image_size': 224, 'lr': 0.0001, 'optimizer': 'SGD', 'sampling': None}. Skipping training.\n",
      "Skipped (model and results already exist).\n",
      "\n",
      "Grid search finished: 0 models were trained, 16 were skipped.\n"
     ]
    }
   ],
   "source": [
    "# Path to the aggregated results file\n",
    "results_filepath = os.path.join(models_dir, \"grid_search_results.json\")\n",
    "\n",
    "def run_experiment(params):\n",
    "    \"\"\"Train or load a model for this set of params; append results to a single JSON.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Params: {params}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Build the unique model filename for this parameter combination\n",
    "    sampling_str = params.get('sampling') if params.get('sampling') is not None else 'none'\n",
    "    model_file = (\n",
    "        f\"model_lr-{params['lr']}_optim-{params['optimizer']}_bs-\"\n",
    "        f\"{params['batch_size']}_sampling-{sampling_str}_size-{params['image_size']}.pth\"\n",
    "    )\n",
    "    model_path = os.path.join(models_dir, model_file)\n",
    "\n",
    "    # Read existing aggregated results, if any\n",
    "    if os.path.isfile(results_filepath):\n",
    "        with open(results_filepath, \"r\") as rf:\n",
    "            aggregated_results = json.load(rf)\n",
    "    else:\n",
    "        aggregated_results = []\n",
    "\n",
    "    # See if this parameter set has already been processed and the model exists\n",
    "    existing_entry = next((e for e in aggregated_results if e.get(\"params\") == params), None)\n",
    "    if existing_entry and os.path.isfile(model_path):\n",
    "        print(f\"Found existing model and results for params {params}. Skipping training.\")\n",
    "        # Include a flag to indicate the run was skipped\n",
    "        return existing_entry | {\"skipped\": True, \"training_time_minutes\": 0.0}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    #   Data preparation  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((params['image_size'], params['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_ds = MalariaDataset(train_json_path, image_path, transform=transform, category_map=category_map)\n",
    "    val_ds   = MalariaDataset(test_json_path,  image_path, transform=transform, category_map=category_map)\n",
    "\n",
    "    sampler, shuffle = None, True\n",
    "    if params.get('sampling') == 'oversample':\n",
    "        sampler = create_sampler(train_ds)\n",
    "        shuffle = False\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=shuffle, sampler=sampler, collate_fn=detection_collate)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=params['batch_size'], shuffle=False, collate_fn=detection_collate)\n",
    "\n",
    "    #   Model and optimizer  \n",
    "    model = EfficientNetDetector(num_classes=num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr']) if params['optimizer'] == 'Adam' \\\n",
    "               else torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9)\n",
    "\n",
    "    #   Training loop  \n",
    "    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_accuracy = 0.0\n",
    "    NUM_EPOCHS = 10\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_loss, train_acc = train_model(model, train_loader, optimizer, DEVICE, epoch, num_classes)\n",
    "        val_loss,   val_acc   = validate_model(model, val_loader, DEVICE, category_map)\n",
    "        print(f\"Epoch {epoch}: Train Loss {train_loss:.4f}, Train Acc {train_acc:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_acc:.2f}%\")\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        # Save a new best model\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            print(f\"New best model! Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'params': params,\n",
    "                'best_val_accuracy': best_val_accuracy\n",
    "            }, model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    #   Update aggregated results  \n",
    "    elapsed_time = time.time() - start_time\n",
    "    # Remove any prior entry for these params\n",
    "    aggregated_results = [e for e in aggregated_results if e.get(\"params\") != params]\n",
    "    aggregated_results.append({\n",
    "        \"params\": params,\n",
    "        \"best_accuracy\": best_val_accuracy,\n",
    "        \"training_time_minutes\": elapsed_time / 60,\n",
    "        \"history\": history\n",
    "    })\n",
    "    with open(results_filepath, \"w\") as rf:\n",
    "        json.dump(aggregated_results, rf, indent=4)\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"best_accuracy\": best_val_accuracy,\n",
    "        \"training_time_minutes\": elapsed_time / 60,\n",
    "        \"history\": history,\n",
    "        \"skipped\": False\n",
    "    }\n",
    "\n",
    "# Define your hyperparameter grid as before\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'optimizer': ['Adam', 'SGD'],\n",
    "    'batch_size': [32],\n",
    "    'image_size': [128, 224],\n",
    "    'sampling': ['oversample', None],\n",
    "}\n",
    "\n",
    "# Prepare the list of all combinations\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "total_runs = len(param_list)\n",
    "trained_count = 0\n",
    "\n",
    "# Run the grid search with progress reporting\n",
    "for idx, params in enumerate(param_list, start=1):\n",
    "    print(f\"\\n=== Hyperparameter run {idx}/{total_runs} ===\")\n",
    "    result = run_experiment(params)\n",
    "    if result.get(\"skipped\"):\n",
    "        print(\"Skipped (model and results already exist).\")\n",
    "    else:\n",
    "        trained_count += 1\n",
    "        print(\"Training complete.\")\n",
    "    # You can also inspect result['history'] here if you want\n",
    "\n",
    "print(f\"\\nGrid search finished: {trained_count} models were trained, {total_runs - trained_count} were skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88825a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
